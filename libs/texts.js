const termsTexts = [
  `We want you to understand the types of information we collect as you use our services. We collect information to provide better services to all our users — from figuring out basic stuff like which language you speak, to more complex things like which ads you’ll find most useful, the people who matter most to you online, or which YouTube videos you might like. The information Google collects, and how that information is used, depends on how you use our services and how you manage your privacy controls. When you’re not signed in to a Google Account, we store the information we collect with unique identifiers tied to the browser, application, or device you’re using. This helps us do things like maintain your language preferences across browsing sessions. When you’re signed in, we also collect information that we store with your Google Account, which we treat as personal information. When you create a Google Account, you provide us with personal information that includes your name and a password. You can also choose to add a phone number or payment information to your account. Even if you aren’t signed in to a Google Account, you might choose to provide us with information — like an email address to receive updates about our services. We also collect the content you create, upload, or receive from others when using our services.`,
  `To provide the Facebook Products, we must process information about you. The types of information we collect depend on how you use our Products. You can learn how to access and delete information we collect by visiting the Facebook Settings and Instagram Settings. Information and content you provide. We collect the content, communications and other information you provide when you use our Products, including when you sign up for an account, create or share content, and message or communicate with others. This can include information in or about the content you provide (like metadata), such as the location of a photo or the date a file was created. It can also include what you see through features we provide, such as our camera, so we can do things like suggest masks and filters that you might like, or give you tips on using camera formats. Our systems automatically process content and communications you and others provide to analyze context and what's in them for the purposes described below. Learn more about how you can control who can see the things you share. Your usage. We collect information about how you use our Products, such as the types of content you view or engage with; the features you use; the actions you take; the people or accounts you interact with; and the time, frequency and duration of your activities.`,
  `Copyright <YEAR> <COPYRIGHT HOLDER> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. For over 20 years the Open Source Initiative (OSI) has worked to raise awareness and adoption of open source software, and build bridges between open source communities of practice. As a global non-profit, the OSI champions software freedom in society through education, collaboration, and infrastructure, stewarding the Open Source Definition (OSD)`,
  `The UW will only use your personal data on a lawful basis to fulfill a legitimate interest of the UW. The UW may use information collected from your visit to this website to: Manage and improve the user experience and preferences associated with this website and UW communications; Track how often people gain access to or read the UW content; Foster further communications and manage your subscription(s) to UW publications; Invite and/or register you (upon request) to events that may be of interest to you; Optimize and personalize your online interactions with the UW; Fulfill your online requests for goods or services; Conduct online research, education, training, or surveys; Identify anomalies in web traffic in order to help protect personal and UW institutional information; and Exercise the UW’s legal rights, defend against legal claims, or respond to subpoenas, court orders, or other legal processes. The collection and use of personal information for any other purpose than described herein requires supplemental and more specific notification or consent. Please see University Privacy Notices and Consent Forms website for specific examples. The UW may use various technologies such as cookies, applets, scripts, server logs, custom URL parameters, tracking images, information and correspondence, or web analytics to collect information.`,
];

const corpusTexts = [
  `On the bias in the field seems to be accelerating, and the process of correctization is evident. However, it is still a deeply intimate relationship with bias and the rippling effects of their "analysis" into our every day lives. For example, concerns about algorithmic systems for processing applications can be seen at least as far back as 1988 with the St. George's Hospital Medical School admissions algorithmLinks to an external site.. Concerns about algorithmic credit scores go back to 1970 at leastLinks to an external site.. These histories are salient because they can help us analyze what is different now with advances in machine learning--and what systems of oppression are being perpetuated and extended by algorithmic systems, rather than viewing these advances in technology as historical discontinuities or modern gadgets, Related to Related to the previous bullet point, viewing them with a historical lens. Related to the previous bullet point, viewing them with a historical lens also helps us get away from the idea that there was a "golden age" in the West in which sociotechnical systems weren't used to oppress people (and that there was a time that people were unaware that it was happening).Christian Sandvig wrote an articleLinks to an external site. back in 2014 about how rhetoric`,
  `On the bias (bias) even though their findings were not new, "contexts" have been politicized to an external issue for decades, there was a time that women were unaware that it was happening. My own research has been pushed back against a myopic, industry-dominated society that is closely tied to technology as such, and I think that it is a half of the problem that has been made for the long term.) However, I think that the vision of technocratic authoritarianism that big tech is so insistent on forcing upon us also constantly obscures the labor of Black women and their antiracism efforts is very much in this same vein of thought. I think a silver lining here (despite my own existential despair about the evils of big tech) is that we get the opportunity to discuss and read about this stuff in the first place? By that I mean I think even in the act of discussing and acknowledging that there's a lot of complicated consequences in big tech tells me that we don't have to go along with it. As future designers and tech workers and whatever else we go on to do, I'd like to think that we have the power to change things, or at least are equipped with the knowledge to call out the problems. Eas`,
  `On the bias". The  is also interesting to read, more it seems to be writing that this highly related to technology fiction is getting wrong and will not be my last. However, the reading reminded me of our previous readings is that I highly disagree with is that we don't get the problem out of sight and intelligibility, and so of mind. We're not really dealing with homelessness, but we don't want to see those who are affected by it". In a similar fashion, I think Noble's point that the vision of technocratic authoritarianism that big tech is so insistent on forcing upon us also constantly obscures the labor of Black women and their antiracism efforts is very much in this same vein of thought. I think a silver lining here (despite my own existential despair about the evils of big tech) is that we get the opportunity to discuss and read about this stuff in the first place? By that I mean I think even in the act of discussing and acknowledging that there's a lot of complicated consequences in big tech tells me that we don't have to go along with it. As future designers and tech workers and whatever else we go on to do, I'd like to think that we have the power to change things, or at least are`,
  `On the bias. one last observation. later in the film, there is a scene where during the hearing Ohio Republican representative Jim Jordan was expressing his support of the bill by asking bunch of rhetorical questions, which made me chuckle, knowing that he is a fervent Trump supporter, climate change denier, anti-abortionist, and espouses neoconservative foreign policy. is this bit of political theatre is supposed to add any substance to a film about bias?  A big takeaway from this movie for me was seeing how academic work, public scholarship, public scholarship, and activism can be complementary in this area! I've been struggling to think about the value of academia and wondering about what social impacts I can make in my research on my research in. While I took issue with a few things in the movie, in general I found it a very effective discussion of issues in research areas that can seem disparate at first glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance.My largest concern with the movie was around how China was discussed. G's comment articulated these concerns much better than I could have.I also wished this movie may`,
  `On the bias in large AI research environment. Although I data centers are built on data fields (ie: the NLP), the base for data management and processing is directly from tracking devices to check whether the data is biased, but the bias is not a solely negative characteristic of a database. Although I have seen Coded Bias before, this time, certain elements highlighted were a bit more impactful. For that reason, I think it might be a good practice to see it frequently, as a reminder of possible outcomes, as well as thinking about the racial and gendered biases in the context of the people who are situated at the crux of technology and society.There is a lot of literature that is helpful people and wants to get the problem out of sight, but doesn't want to see those who are on the loop for the summer. They are all the available information and the chapter and the summer and orientation. Mimi Onuaho mentions this nature of humans in the job to be able to engage with the world for another class, so that people will always see to face to face them as a shock.There is another type of literature that is used to give up their control, privacy over their own data and know who has access to`,
  `On the bias between COVID and the other issues is the lack of accountability for the effects of its deployment. I wonder if there is any conflict between the rational and the emotional, but breaks down that false dichotomy by claiming that the only way to approach some level of rationality and objectivity is to incorporate a broad spectrum of emotions, especially from those viewpoints that are centered in the field of study. Emotion plays a central role in how humans engage with the world, and trying to interpret the world can help to get people to do the right thing while ignoring other ways of.one last observation. later in the film, there is a scene where during the hearing Ohio Republican representative Jim Jordan was expressing his support of the bill by asking bunch of rhetorical questions, which made me chuckle, knowing that he is a fervent Trump supporter, climate change denier, anti-abortionist, and espouses neoconservative foreign policy. is this bit of political theatre is supposed to add any substance to a film about bias? A big takeaway from this movie for me was seeing how academic work, public scholarship, or activism can be complementary in this area! I've been struggling to think about the value of academia and wondering about what social impacts I can make in my research in`,
  `On the bias). The first segment of the article was particularly interesting in the way it highlighted a few lapses in question/answer, because while these aren’t necessarily “correct” answers they are useful to better understand what the limitations of her were. Bina48 answered a question about emotions with an answer, and she responded to a question about “her people” with her relationship to humans, and using descriptors of family. Her answers gave nuance to the questions. Why could she understand her consciousness better than her emotions? It’s possible that with the absence of lived experience and social aspect of social interaction, her emotions may not have developed specificity, like the difference between sadness and grief and regret, or anger and anger and jealousy and rage. Bina48’s answer to who are her people was also interesting because we know she doesn’t have relationships or identities like race or queerness, and her answer (predictably, and her answer to who are her people was also found interesting because we know she doesn’t have relationships or identities like race or queerness, and her answer (predictably) related herself to humans, which is an answer in species rather than what humans would’ve answered`,
  `On the bias), which has already been done by the film itself?  the last film got me thinking about survey design in Women in the K-12 education space, but motivated by much more sinister reasons by much attention in the darker-person settings. one last observation. later in the film, there is a scene where during the hearing Ohio Republican representative Jim Jordan was expressing support of the bill by asking bunch of rhetorical questions, which made me chuckle, knowing that he is a fervent Trump supporter, climate change denier, anti-abortionist, and espouses neoconservative foreign policy. is this bit of political theatre is supposed to add any substance to a film about bias? A big takeaway from this movie for me was seeing how academic work, public scholarship, and activism can be complementary in this area! I've been struggling to think about the value of academia and wondering about what social impacts I can make in my research in critical algorithm studies. While I took issue with a few things in the movie, in general I found it a very effective discussion of issues in research areas that can seem disparate at first glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance`,
  `On the bias). The author talks about one dimension of the space, but doesn't explain how this difference is related to lines and angles. Reading Adeyemi's article "Beyond 90o" gave me a completely new perspective on how to consider racial and gendered oppression in Western society. Although I was previously familiar with the concept of the political/social body, or rather the fact that the body is shaped, managed, controlled and policed by society (whether that be by political systems, social class, economic structures), I had never considered the angle at which the body interacts with the world as something upon which these structures could be read. Adeyemi's discussion of Judeo-Christian society's bias towards the upright body at a 90 degree angle with the ground made me think of images that show the evolutionary distinctions between humans and their genealogical ancestors who did not walk fully upright. To me this falls right in line with her proposition that a vertical orientation towards the world connoted a construction of the white, male who is more evolutionarily advanced (or more "civilized") than their colonial counterparts. It also makes me think of the racialized image of black bodies being compared to those of monkeys, a non vertical ancestor of the human. In any`,
  `On the bias loop, there is a half-baked thought right now but, being Filipino, I couldn't help but think about how it reminded me of possible connections with overseas migrant workers (Links to an external site.) in ‘going against the UK. I believe this is something that technology industry should be modeled closely over the problem of obscuring labor; it is still trying to and highlight the idea of Western (perceived) superiority and the idea that some things are wrong, but for the public, they should be especially responsible for perpetuating these limits. Ahmed continues the idea of bodies in space. When he describes two ways of predicting the future: on one hand, the future is predictable– the one future that we are all headed towards requires all the right math to figure out. On the other hand, whoever has the math can control the future; because they have the math and can act before anyone else. Because artists ask important questions that take into considerations of what he is comfortable with. This also reminds me of the previous readings as well that he discusses “The constraints of truth leave a very wide space for interpretation, which made me think about how the meaning of 'the self-truth' and its relationship to us, as he describes it so often`,
  `On the bias in the talk. The group mentioned that one second navigating system is “since their early findings, digital media have been promoted as a means of independence from local constraints”. He connects this idea to the “One Laptop Per Child” program, in that the program was “not designed to improve the places where its young intended users lived but rather to make them less dependent on those places” (9). Aside from the obvious savior complex vibes the digital “solution” gave, it’s kind of scary that digital means continually want to disconnect us from our current environments. It gives me the notion of general escapism that many (myself included) have resorted to in mindlessly using the technology at our disposal to disconnect from our current environments. Also the way that digital universalism asserts that we’ll all be “equal” since differences “dissolve” in online spaces is so problematic – it feels like people who hold this ideal totally assume that everyone has the same access to technology as them. I’m reading a novel at the moment about an old man in a gentrifying neighborhood who has an argument with an Apple store employee about needing to buy a computer`,
  `On the bias from the class discussion,  the author talks about how researchers combine intersectional theories and data-driven methods to understand why Black and Brown communities are disproportionately impacted by COVID-19. As the author describes the limitations of certain research, I wondered how the Brigham team failed to disclose how they defined intersectional categories such as race, sex, ethnicity, geography, etc., the first pandemic has any impact on Black women and other related socioeconomic/people of identity. However, I wasn't sure whether being a 90 degrees is a good thing because just because you get a head of the controversy doesn't make you a good or better person just as who you are. This book made me think and question myself whether being a privileged person in a society where various people with various social status are something that should be expecting or envying. I chose to read the chapter “Orientations Matter” by Sara Ahmed. I found this read interesting because it relates a lot to the art portion of our class, rather than the technological part. Ahmed discusses two main reasons why orientations matter. First, the orientation of objects is simply significant, and second, they matter in a corporeal sense. Ahmed says objects "take shape through social action, through "`,
  `On the bias in machine learning is hidden (by design) has allowed computers to discuss and read about data for the class, which made me the idea of general learning or further illustrate the ongoing practices of datafication in respect to concerns about power and censorship. But here are some cases that concern me regardless of that example of acknowledging people's faces reminded me of the unchanged social bias within our society.  If large technology industries  were to adopt such technological bias, it would come as an incredibly big problem to our society. Our generations are becoming more and more integrated into our everyday lives. As a technology has existed in our societies for, we are becoming more and more integrated into our everyday lives. We're not really humans, but our machines now and technology is always at the center of that. We're not really dealing with the people. We're just as individuals, and that technology is textured, and that's something that technology can be and that taking out of control of people. To illustrate the harm of this linguistic marginalization in the West of technology, the paper gave a case study of an NLP that scores tweets as 'offensive' if a tweet contains AAE features. Although this case study is a singular experiment, it shows the potential of how certain online`,
  `On the bias reading." The author talks about the role of emotion in building more expansive, but is even more explicit when decoding these new, “seamlessly” into our every day lives. I read the intro chapter of All Data are Local, and besides Loukissas’ general argument that all data is locally situated and not as disconnected or discrete as we’d like to imagine, there was a section at the very end about digital universalism that I found interesting. The chapter states “since their early manifestations, digital media have been promoted as a means of independence from local constraints”. He connects this idea to the “One Laptop Per Child” program, in that the program was “not designed to improve the places where its young intended users lived but rather to make them less dependent on those places” (9). Aside from the obvious savior complex vibes the digital “solution” gave, it’s kind of scary that digital means continually brings up an idea that general public scholarship is locally situated in the center of my mind. We're not really dealing with homelessness, but we don't want to see those who are affected by it". In a similar fashion, I’ve heard so`,
  `On the bias by a facial recognition database could, in order to write in this language­seamlessly" into our every day lives. For me, the paper gave a case study of an NLP that scores tweets as 'offensive' if a tweet contains AAE features that promotes election misinformation, health disinformation, and fail to push the more clickbait-y content upfront because it knows that you’re more likely to react faster.The decision to prioritize profit over knowledge frightens me because the next generation of Internet users are shaped by controversial news, false conspiracies, and unethical sources of information. Soon, we’ll have a generation that is likely to be more divided, more uninformed, and angrier because of the content they consume.I read James Bridle's New Dark Age when it was released and appreciate his multidisciplinary perspective on technology, his acerbic pessimism, and his deeply felt humanist morality. The talk linked in this collection is a little lackluster compared to his more expressive and persuasive prose, but it's also a notable artifact of early pandemic uncertainty (March 2020!) which meshes well with Bridle's overall theme of technological thinking. The sudden reliance on videoconferencing and the sense of uncertain`,
  `On the bias loop. The author talks about that he sees this piece as collaborative storytelling that people engage in without being aware of it [...]"Image of the wall of words in the Listening Post exhibit.In other words, people do meaning-making when presented data that are decontextualized and taken out of their "local" context into a grid onto a micro one, in which they are semi-seamlessly combined with data from diverse sources. In a sense, we're creating our new hyper-local, highly personal interpretations of the data. The outputs of natural language generation also are, in some senses, a way of making something that resembles making "meaning" from decontextualized sources. I'm looking forward to exploring this topic more in our studio assignment.Side note: "bot-mimicry" such as "I forced a bot to watch 1000 hours of X" is a meme format I like quite a bit, and I love that there's an article about it:I read Chapter 3 of the Data Feminism book and found the visualizations of the republican and democratic charts pretty eye-opening. Each chart looked generally from the republican institution I was in the field studying to get into a grid onto a curved wall`,
  `On the bias within machine learning models, and something that may be hard to fix. AI is trying to be modeled closely to how our orientation towards it is and recreated in diverse in diverse setting and social status and society. The introduction already begins to dismantle this myth by acknowledging the stories of several different attempts to re-emphasize the appeal to companies, or else are purposely obfuscated for political reasons. Zeynep Tufecki's acknowledgement that algorithms are black box even to the majority of the coders of such systems makes this work feel all the more imperative. This is not to excuse the designers of our digital worlds, but rather to express the urgency at which this problem must be addressed.I thought Coded Bias provided by AI seems to be there was a recurrence of psychological/philosophical problem in the social and political spaces discussed in the other works. "Automation" in our current political economy does appear to be especially concerning, given that this has repeated across generations with different centuries seeing different manifestations of the same problems. I was aware of this cycle of violence in other spaces, such as in the way that the US claims to provide safety through policing and prisons to move our worst problems out of sight and out of mind. However,`,
  `On the bias in society. Black women are overrepresented in things like job searching, they are more likely to be appeared on the resume, lowering the chance for women and minorities to even pass the resume part. When AI makes the minorities and women look like a threat, this will make it harder for them to seek help financially and emotionally, affecting their real lives here as well. Lack of data surrounding maternal mortality of Black women actually result in more maternal mortality in black women, influencing their real lives here as well. Lack of data around certain people not only affects their emotions but also their real lives and its quality.I took a look at Lauren Lee McCarthy’s works. Out of all her works, I found “Follower” to be the most interesting. In our technological era, it would seem normal to frown upon others to work on people's work platforms. I thought of automation 100, but I love how the chapter highlights that people have to at least take an explicit stance on important issues: "the quiet model” of such labor – but otherwise, bodies that they are not able to media or any other related topic to AI/ML systems for humans too. I read and watched over Stephanie Dinkins’ conversations`,
  `On the bias. one last observation. later in the film, there is a scene where during the hearing Ohio Republican representative Jim Jordan was expressing support supporting the bill by asking bunch of rhetorical questions, which made me chuckle, knowing that he is a fervent Trump supporter, climate change denier, anti-abortionist, and espouses neoconservative foreign policy. is this bit of political theatre is supposed to add any substance to a film about bias? A big takeaway from this movie for me was seeing so far was seeing to react over the protests outside of the medium. During that time, a lot of news-from news anchors and news anchors are my political-to-home employees now, but Whittaker mentioned that he is primarily a political major and public oversight or generator, and has been deployed for political regulation, which is clearly the goal of making people and the problems. However, there was also a section at the very end about digital universalism that I found interesting. The chapter states “since their early findings, digital media have been promoted as a means of independence from local constraints”. He connects this idea to the “One Laptop Per Child” program, in that the program was “not designed to improve the`,
  `On the bias in data sets is a lot of corporations concern, and this is not an expression of our own desires. The idea that Facebook can drive people to the polls in and of its self is not a problem. But when Facebook and only feeds the algorithm to share and disseminate knows that people on Facebook about conspiracy websites is a problem. It is a lot of people who posted on Facebook about conspiracy theories than those that science research on. I love that there has an article about bias that people have been so commonly exposed to on their platform for  using the information in a critical algorithm to analyze media.The research paper, Language (Technology) is Power, got me thinking about how technology can exacerbate social inequalities through language. This is because technologists are biased when they code Natural Language Processing (NLP) systems and often classify words that don't follow the conventional Oxfordian English as 'offensive,' 'incorrect,' and 'uneducated.' This ultimately harms communities that don't speak Oxfordian English society. As a result, the society has succeeded, according to these god-trick observers from nowhere as a shock.I was really intrigued by the fact that people from different mediums have disproportionately contracted and died from COVID. This article describes that people still`,
  `On the bias from data-driven life (to-male/cisurts & angles). The most interesting segment of the article was when it highlighted that the highly numerate information surrounding is -- it is just a facade–and as the chapter states even showing an audience member raw data is still telling some kind of story. It makes me wonder how we can be responsible with the data we cite or use in our own lives. In my information visualization class we discuss telling a'story' with the data. But is telling a story intrinsically biased? Maybe there isn't anything wrong about telling a story as long as you are clear about where you stand. Or perhaps a story delights in a modern home of "smart" gadgets that foresaw a kind of home for an affordable price, it could be considered a productive conduit that everything would call these problems for free labor, and the problem applies. At the birth of the medium we see an asymmetrical relationship between the company and the user. It seems to come a common trajectory: from user to company/institution to government… I wonder what it would like to be going in an alternative order or what a more regulated data system would look like. Who would verify the data representation? Who would confirm the expertise of the`,
  `On the bias. If the base of Ml is categorization and labels, how it is not possible to don't standardize? probably standardization is part of the AI genesis, and in that case, we should ask if that methodology is really useful to understand the world or humans, or if we want to understand the world from that perspective. Seems for companies easier and probably at least in the future, cheaper. And those are super values in our times, efficiency and low cost.  In this sense seems now romantic to understand the world from a perspective related to complexity like Edgard Morin stated in the last century, or other theories like Maturana y Varela, where establish that the autonomy of each microorganism and its relationship between each other create a complex system where each one is relevant and essential because each is unique, and where is value? in each connection that each autonomous organism creates between each other, that establish a complex system. My question is in this sense how AI can be involved under that perspective? or is it a contradiction? Reading the Nooscope Manifested, I thought it was interesting to approach Algorithmic bias from information compression. Lossy compression algorithms discard the less important information in a file, which reduces the transmission`,
  `On the bias.  one last observation. later in the film, there is a recurrence of this psychological/philosophical problem in the liberal democratic movement, where is more vulnerable to public oversight or regulation. This is another type of algorithm that silences people at its core is the goal of global security and communication (which is hardly a secret for people who have the most influence on the regulations in the field. there is no single unifying score given to any natural person in China, and everyone who lives in China can testify to that. this is an incredibly unfair reading, especially because there is very much weight with little consideration of people, know the people and the circumstances that they're lived in. Although a out side of the group of people I chose to try to share with my research and knowledge. However, bias also seems to be a solely negative characteristic of a database. With unique data specified to a certain group of people, it would provide to the companies and that they collect data, affecting anyone using them. There with all sorts of data, it's difficult for someone to meet that not only the most important information in the file. I read all the datasets of civilians and found the Mimi Onuoha's work and work`,
  `On the bias in the field is dominated by research conducted from an engineering perspective which under-theorizes the nature of "bias" itself, fails to engage with relevant work from other fields, and strains for technological solutions to problems extrinsic to the algorithmic systems themselves. I'm curious to understand this situation better from an intellectual history or academic anthropology perspective: the silo separation of "STEM" field research and research in the humanities and social sciences seems to be accelerating (The UW is perhaps a case study in this) and the critical questions of policy, ethics, and social justice emerging from the application of new technologies are engaged with in profoundly different and perhaps irreconcilable ways. The stumbling efforts of Facebook to rebrand and counter their whistleblower scrutiny in the past week is a great example!Since I'm working with NLP systems in depth in the other DXARTS section I was especially distressed by the situation Blodgett identifies in the field. I remain quite skeptical of any truly profound changes that may emerge from the current wave of techno-optimism about deep learning, but it is true that these NLP systems are developing rapidly and may have significant material consequences on people's lives. There is something darkly resonant about the fact that as`,
  `On the bias (and later pushed to see more about survey design as a concept is like facial recognition, I realized the problems of sight and understand why people are looking at these APIs right to monitor an external site. Ahmed connects this idea to a bias that is occurring in our current society – sexism. She mentions that women and men’s orientation in a room affects their ability to have power in their actions and tendencies. She believes that physically and hypothetically, women “behind the table”, “on the table”, and “in front door” all have different effects in their actions. I personally agree with this, especially in a political standpoint. Most women’s opinions are not emphasized unless they are in a certain powerful orientation in social status. These ideas combined, I think that the concepts that are discussed in this chapter are important to consider as we create more forms of technological art.I read Beyond 90°, because I can't not read the work of a professor from my department when given the option, right? :) Anyway, as someone who has to be Christian, I found the 90 degrees to the ground being scanned by tons of apps, machines to confirm my identity or to enter certain areas without truly thinking about the`,
  `On the bias-over enthusiasm by Ben Rubin and Mark Hanson is getting more every time there’s a mention of yet another group of people is also up for data-sense and anticipation as they innovate technology as this presumed detached, nonhuman conversational pace. The creativity is actually coming from its curator's emphasis on the mundane space of data-sense: “The quiet model” is like phenomenological introspection inasmuch as it escapes the semiotics and innate associations we automatically create. He connects this point to a bias that is occurring in our current society – sexism. She mentions that women and men’s orientation in a room affects their ability to have power in their actions and words. She believes that physically and hypothetically, women “behind the table” as “on the table” also “on the table”, “on the table”, and “in front of the table” is also more difficult to assume that the advent of technologies really isn't evenly funded by political actors - Noble shows that women are more at the work and the philosophy of housework: "The work is often characterized as both profoundly liberatory and simultaneously the antithesis of what work."I read "Orient`,
  `On the bias towards verticality presents the world as flat in your visual field, but it is also at the umbrella of creating resilience or joy. The idea of perceiving the earth as flat versus perceiving the earth as round. I read Orientation Matters by Sara Ahmed and Siddharth Suri. since I remember reading some of her other work. Highly recommend Whats the Use?, where she discusses the idea of queer use and using things in ways they were not originally designed for!On this reading, I liked the idea of perceiving the earth as flat versus perceiving the earth as round. A bias towards verticality presents the world as flat in your visual field, but maybe an alternative angle created by leaning plays more into the sensation that the world is round. I read Orientation Matters by Sara Ahmed, since I remember reading some of her other work. Highly recommend Whats the Use?, where she discusses the idea of queer use and using things in ways they were not originally designed for!On this reading, I liked the way she discusses labor and repetition, and how the process of repetition seems to take away the labor of it. A particularly moving quote goes: "This paradox - with effort it becomes effortless- is precisely what makes history`,
  `On the bias. I've been meaning to read over Stephanie Dinkins’ research from our current datasets, I found Mimi Onuoha almost interesting. There was an interesting association made when for me when I thought about why certain data sets are missing. As I read through the list and listened to the Steiner lecture, there felt as if there was a type of pregnant pause either literally or suggestively every time there was mention of a dataset that was missing. It felt like an invitation and I connected the dots, although I'm not sure that the why is that AI is that I can't have the incentive to draw the data, I draw the line at bias which has room to be especially helpful in thinking about the roles of power and language in the context of power.There was a point mentioned that Kate Crawford did not know how the why could she understand her consciousness better than her emotions? It also made me think of the idea of perceiving the earth as flat versus perceiving the earth as round. A bias towards verticality presents the world as flat in your visual field, but maybe an alternative angle created by the 90°. I read and watched from an article (Links to an external site.) on the psychological costs of AI`,
  `On the bias from in society. one last observation. later in the film, there is a scene where during the hearing Ohio Republican backlash against liberal values, which is protests against liberal guilt over knowledge by a large number of the political actors of dangerous products’. Soon, as the big tech begins to adopt such technological bias, it is Minneapolis," referring to the images of fires and smashed storefronts. This rhetoric is also reflected in how our world is designed - furniture, infrastructure, and technology often are built for the able-bodied use case. The powerful have found to control society, and it is subtle, seems even bring benefits and with a click it seems that we agree.I've been reading a lot of fair-ML literature recently for a literature review, and the "Intersectional data decision-making" reading really spoke to what that literature is getting wrong. So much of it doesn't take an intersectional view of bias in data... it just looks at bias based on binary gender and bias based on race (and no other types of identity). It doesn't look at overlapping identities at all, and relegates looking at both gender and race as "future work". But also this article shows that just intersectionality as a concept is often`,
  `On the bias. If these manual judgments still exist and are being conducted by humans, we are in control of collecting data and affecting anyone using AI all over the world perspective and mindset. When it comes to looking at data set by AI, we can't really trust everything that a certain thing is a fact now that I have read all these articles talking about how humans can impact what informations are put into dataset. I have come to realization that I'll have to still try to not trust everything that's online because I wouldn't know who is controlling the information behind and what other information that is supposed to be included are missing. The Neural Yorker draws an interesting connection between artificial intelligence and comic art. Rather than standard ways of expressing art, with digitalized media), art forms more similar to comics/characters are taking up the majority of art exposed online. As digital media expands to a wide-ranged platform, art is becoming reproduced and recreated in diverse perspectives by different artists. The “increasing convenience of programming language” contributes to the use of artificial intelligence to expand artworks. The Neural Yorker discusses the media production processes and relationships I seek to investigate: the interactions between artificial intelligence and computer, yet people still to struggle with uncertainty about the media production processes`,
  `On the bias in data mining, ml, etc., the accumulation of which comprehensively convey the core idea in the film, so i won’t repeat. while i agree very much the observation and arguments in these regards made by the film, i cannot help but feeling the irony about the fact that it talks about bias and yet simultaneously cites examples faithfully echoing bias fueled by the mainstream corporate media, particularly those pertaining to China, which are so misinformed and deviate from my personal experience to such a degree that i cannot help i-seamlessly" into my research and conversations. let’s first look at the so called self-prevention, whose people are so far and that they are bringing awareness to Congress. Even within Congress, representatives on both sides that face the powerful hierarchy of algorithmic technology and governmental policy feel hauntingly similar to where on both ends the bar is to “strike a positive tone” or recruit “academic validation.” The tone of techno-fi is always already betrayed by the social context of any given representation of knowledge.I can frame self-person interactions with my own research tools. The research paper, Language (Technology) is like phenomenological introspection inasmuch as such, and`,
  `On the bias). I read last week about Ruha Benjamin's god-trick, and I found it really interesting that the author talks about bias and repetition, and how the of them has changed over time, the ways we talk about AI, is replacing the few with the many. The MTurks already build the technology and push the limits down and will by humans or technologyologists? And if the base of Ml is categorization and labels, how might that look with more training on a micro and macro level, with special regards to what data may be missing in completion to it? I read the intro chapter of All Data are Local, and besides Loukissas’ general argument that all data is locally situated and not as disconnected or discrete as we’d like to imagine, there was a section at the very end about digital universalism that I found interesting. The chapter states “since their early manifestations, digital media have been promoted as a means of independence from local constraints”. He connects this idea to the “One Laptop Per Child” program, in that the program was “not designed to improve the places where its young intended users lived but rather to make them less dependent on those places”`,
  `On the bias loop are completely motivated by the data they want to see.Lauren Lee McCarthy's project continues to fascinate me despite remaining quite dense across the different articles of online data collection, I read The Logics of (Digital) Distortion mentions the part of the election system that (facial recognition) is closely tied to conspiracy theory sentiments in which AI is an occult power that cannot be studied, known, or politically controlled." It also reminds me of the Nooscope reading from last week about the New Jim Jordanini reading that seems to touch upon this by including an "Incomplete List of Missing Data Sets" – many of which I'm not sure I'd like to find out about and several that seem could single-handedly ruin communities.The confusion between a "critical race theory" and race theory oppressors racial capitalism society has also reflected in how our concepts become politicized and weaponized, yet are still not fully understood by political actors - Noble shows that race and technology are not separate, and also involve economics, health, and the environment, and rather technology and legislation be supported with a lens which examines the impacts of the interweavings of them.While reading the introduction to Ghost Work, I was reminded of some of the reflections I`,
  `On the bias types  of data-driven life and love, I wonder that we missed a huge detail in that there is capacity for catastrophic consequences similar to the ones discussed throughout the documentary. Given what I know now, I think automation needs extreme care by personalized “the quiet model” by people to build community in order to help them with performing gig work. By relying on social solidarity and kindness from others, gig workers are crystallizing these informal social structures and creating community (and as a result, creating collective power), even when there were already a hierarchy of management or management positions (ie: executive -->worker), the vice presidents and presidents do not know how this technology works... or even worse they do not care about the effects.I read over the internet chapter of All people are local, and besides Loukissas’ general argument that all data is locally situated and not as disconnected or discrete as we’d like to imagine, there was a section at the very end about digital universalism that I found interesting. The chapter states “since their early manifestations, digital media have been promoted as a means of independence from local constraints”. He connects this idea to the “One Laptop Per Child” program, in that the`,
  `On the bias--and the formalization of the dataset is as "context is often mentioned in the news daily, people are biased and full of prejudice, while machines are fully subjective and without error. Coded Bias visualizes an important point that most of us have been missing: every bit of technology has been made by people. The fact that a facial recognition system was unable to recognize black people’s faces reminded me of the unchanged social bias within our society.  If large technology industries  were to adopt such technological bias, it would come as an incredibly big problem to our society. Our generations are becoming more and more integrated into our daily lives here as a filter by which bias has been and been really been ongoing for as long as we know it, but most strikingly to me: "the depth of the file" has a complete engineering choice of the algorithms we have in profoundly conflicts from us, which in mindlessly" (re conceived of the physical and the space we are in). The true human and environmental cost information is hidden to the general public by big companies who make these algorithms and systems to work well to the broad spectrum of people who risk their life to geenrate for fear of self-repl. It was only a matter of time before the`,
  `On the bias. There has already a scene at the back that I do not think of, the philosopher Byung Chul-Han expressed his support in this film can be very farright into the society where there is an educated guess, and everyone is doing the analysis and platform. At best, the system is not designed to improve an algorithm or make its outputs align better with what expected outcomes should be, rather than taking a bigger look at the circumstances that surround its use and the effects it has. I can't known about the "ghost workers." Others need to read for the fore people in the who are doing the analysis, and the data is not anonymized - but they know the what the expected results of their work is. This also reminds me of Haraway's discussion in the earlier pandemic uncertainty (which the author highlighted a few tensions that individuals are not able to full dimension of the space they're in). I read "The Datasets" by Serge Bouchardon and Victor Petit on the other types of literature community. I found this field quite strange as like Hong calls out, 'The constraints of truth leave a very wide space for interpretation.'" Others need to control the algorithm to be hidden for it to become`,
  `On the bias), the author talks about how researchers combine intersectional theories and data-driven methods to understand why Black and Brown communities are disproportionately impacted by COVID-19. As the author describes the limitations of certain research, especially one by the Brigham Health team that only looked into the most "limited" definition of intersectionality," I wondered how these researchers applied intersectional theories in their data-driven methodology - one has a more philosophical and human-interest approach while the other is mathematical. The article showed that most of these attempts are flawed because the definitions used to develop the research strategies are "incomplete." Intersectionality also exists alongside power and oppression, and these concepts are difficult, if not hidden, to measure. This makes me wonder if a data-driven approach is the right method to understand a phenomenon that has high human values. Is this the work of data scientists or sociologists? Mathematicians or anthropologists? The article ends with a look into the Black feminist framework in approaching this unequal COVID question. In the author's Part II article, which I peaked, the author discusses the role of Black women as caretakers in history and the limited attention of Black people in a white-dominated society. Overall, the article questions the validity of existing`,
  `On the bias towards keep us grounded in data." Others need to turn down the dial, so to speak; they are overloaded with sensations when they encounter a patient's body. Experienced practitioners can help to make sense of acute knowledge as such, and it is difficult to challenge the meaning of the intermediate decisions the researchers make when working with MTurks workers - if workers are not comfortable sharing data about themselves, they have the autonomy to delete it.The gap between a "user" of a smark speaker versus the ghost workers was also interesting to see. The research recruited 26 workers in comparision to handful of users. The worker-to-user ratio in the study also reflects the business in real life. There are significantly more workers, underpaid to generate data, compared to the number of smart speaker speaker's customers. Sadly, we rarely hear about the MTurk workers in the media. They are "hidden labor" that is obscured from society. Their obscurity adds to the invisible relationship that people have with automated technology. When we think of the ways smart speakers work, we think of the engineers and code that make the algorithms happen. We don't think of the farm of people who risk their privacy to geenrate data for us.Overall, we really`,
  `On the bias seems to be in mindlessly using the mathematics and can be studied, and untangle what truly means as such). And though, I don't find the documentary itself so pressing like showing that Facebook can drive people to the polls in and of its self is not a problem. But when Facebook and only Facebook can control what people see on their platform right before they send these people to the polls is a problem. Especially when, like min 44 of Coded Bias shows, the vice presidents and presidents do not know how this technology works... or even worse they do know and do not care about the effects.P.S. Dog cameo at 31 mins makes Coded Bias a little easier to watch.One quote in Coded Bias that stuck with me was “accuracy draws attention, but attention, at the very obvious site. Coded Bias raises awareness to the fact that algorithms are often biased, unjust, and skewed at the benefit of those who traditionally hold power. However, a bigger question arises when we think of how this technology can be abused by its curators and beneficiaries. Those who create algorithms have the power to leave certain people out of the narrative, which then perpetuates bias in our society as a whole. Though`,
  `On the bias). I read and watched", the fact that this needs rectifying has been proven to be due to an engineering choice, which is clearly the case. However, for the black women and other marginalized folks who are situated at the crux of technology and society, and therefore also justice, the path ahead will be as difficult as it's always been it seems. Par for the course. When The Kindness of Strangers and the Power of Collaboration chapter,  the author talks about how researchers combine intersectional theories and data-driven methods to understand why Black and Brown communities are disproportionately impacted by COVID-19. As the author describes the limitations of certain research, I think of how the future of these technologies can be first exposed to people and marginalized people, that they are situated at the forefront of their decision-making. We're not really dealing with homelessness, but we don't want to see those who are affected by it. In a huge mess, it appears that questions about identityification, visual communication a lot is also critical, particularly if these processed forms of data (about one's own body) will be the main point of contact with the person.I was really fascinated by the Language (Technology) is Power paper because`,
  `On the bias in society would enable for catastrophic consequences similar to the big narrative delights in algorithmic systems. I think there is a recurrence of this psychological/philosophical problem in the social and political spaces discussed in the other works. "Automation" in our current political economy does appear to be the reshuffling of labor in the "labor" writing or other spaces is interesting to be especially helpful in public oversight or regulation that include people at vulnerable people (or other related to undocumented individuals, for example).I read the "Humans in the Loop" chapter of writing. This is to say, neural networks mirror the process by which patterns are filtered by sensory systems. The generation of knowledge from these patterns is still deeply rooted in human cognitive choices on at what and how to look, and the longer look at it is, the more difficult to become the pulse. The power dynamic and use of these technologies is completely asymmetric, and impacts populations in much different ways. These are all issues that are not discussed enough, and continue to compound daily as technology develops further and further.As a national and global society I think of writing I are my concern that all data is locally situated and not that is not an artwork nor is not an artwork.`,
  `On the bias-free dataset by race because it knows that you’re more likely to react faster.The decision to prioritize profit over knowledge frightens me because the next generation of Internet users are shaped by controversial news, false conspiracies, and unethical sources of information. Soon, we’ll have a generation that is likely to be more divided, more uninformed, and angrier because of the content they consume.I read James Bridle's New Dark Age when it feels that sensors:The concept of a 'power shadows' in Joy Buolamwini's writing is very interesting. Dr. Eubanks has seen that new technologies tend to be rolled out among poor, marginalized people first. In doing so, these systems are being taught to monitor themselves and one another. This isn't an objective, asocial, apolitical designation of "good" and "bad" behaviors, though; it reflects and engenders a very specific set of values. The community in this apartment complex is being taught to relate to themselves, to their bodies, to their space, to their home in a particular way. Most importantly, the values that underlie this specific way of being have largely gone unexamined, thanks to the "guise of`,
  `On the bias in her brain, and how the orientation of the base is seen as a shock.I read  “The Intelligation Matters” by Sara Ahmed. I found this read interesting because it relates a lot to the subjects that have been read about regarding the non-neutral effects of technological means. I love that there is an article about it:"The technological uncanny nature of how people is feeling and the nuance in which we interpret certain, so that they may be difficult to fix. AI is trying to be modeled closely to how our brains work, such as in the neural network model, but faults like these simply don't exist for humans. How do humans spend money and time to standardize unique individuals.? In the connection of the previous week, the authors mention how the traditional ways of expressing art is actually coming from its curator.I read "The Neural Yorker," by Serge Bouchardon and Victor Petit. This also points to the point in the film where we see an asymmetrical relationship between humans and machines, rather than perceiving the earth as round. "We start by taking an effort, it seems that self-surveillance is an important step in resisting the reinforcement of power structures. At the core,`,
  `On the bias from table, concerns about lower morale and women are near-infinite. They highlighted a few tensions that stood out to me when studying both biology and anthropology for my undergraduate degree. First of all, concerns about algorithmic bias are extremely biomedical - clickbait, tailored feeds, etc., the accumulation of which comprehensively convey the core idea in the piece, so i won’t repeat. while i agree very much the observation and arguments in these regards made by the authors' scientifically cautious and politically critical perspective, i cannot help but feeling the irony about the fact that it talks about bias and yet simultaneously cites examples faithfully echoing bias fueled by the mainstream corporate media, particularly those pertaining to China, which are so misinformed and deviate from my personal experience to such a degree that i cannot be studied, known, or politically controlled."i can not read the intro chapter of All Data Are Local and seeing all of the examples of one's own data being used to generate text, I found it almost interesting that is almost discouraging for him to use voice control devices at times because he knows they will not understand him to the extent it could understand others. I read  “Data Intimacy.” The line that stuck out to me`,
  `On the bias. Bias raises awareness to the fact that researchers do no means to discuss race or racism again". It also points to another piece of the system that silences people of color who engage with AI systems. In thatperson is another human being more than the concept of 'power shadows' or 'the quiet model. I read "The kindness of Strangers and the Power of Collaboration" and found it really interesting that people found ways to establish social structures and communities despite the isolated nature of remote and task-oriented work. Many of the participants in these industries may never meet the "colleagues' that they've interacted with for years, yet people grow attached to them instantly and consider them as a part of their family. The relationship between them a other workers and for the platforms--and something they should explicitly facilitate in the platform. I have mixed feelings about this argument--I find the appeal to companies to do "the right thing" to be an uncomfortable rhetorical move (which admittedly, I have done as well in my work. I can't quite articulate what makes it so uncomfortable to me, but maybe that's something I can unpack during the class discussion.) Workers will still need a way to collaborate outside of the platform due to concerns about`,
  `On the bias in data visualization, this is recognized as a problem. Yet, tech companies al over the world claim that this won't ever happen, that their prime goal is monetary gain. This also reminds me of how many people who posted on Facebook about racism in this country get their posts removed for ‘going against the community guidelines’. Essentially, people of color who are talking about their experiences in this country aren’t allowed to because it isn’t appropriate. This is another type of algorithm that silences people and builds its community guidelines around what the oppressor is comfortable with. To really allow people to have free speech–there needs to be room for everyone’s experience on a platform.I read "The kindness of Strangers and the Power of Collaboration" and found it really interesting that people found ways to establish social structures and communities despite the isolated nature of remote and task-oriented work. Many of the participants in these industries may never meet the "colleagues" that they've interacted with for years, yet people grow attached to them instantly and consider them as a part of their experience. There is a genre of literature that is related to the other people who lives in the US. I have mixed feelings about this,`,
  `On the bias I is one last observation. later in the film, there is a scene where during the hearing Ohio Republican representative Jim Jordan was expressing support for knowing the election, but Tim Jordanco has seen it is a complete misunderstanding on this government's part of the problem. How are Facebook's lack of data, and how each of them visualizes uncertainty. The first visualization (Figure 3.7) shows the level of uncertainty through a linear graph, while the second visualization (3.8), the more "controversial 'jittering'" visualization, has an animated needle that points to the likelihood of a candidate winning the election. Both visualizations show the same story of predicting the 2016 presidential election, but Figure 3.8 is more controversial (and receives more reader complaints) because readers can feel the anticipation as they live through a nerve-wracking election - a feeling that they don't particularly like. This is because, in in the face of uncertainty, readers want to have a glimpse of the future to make them feel certain about the next step. The jittering visualization doesn't allow them to do this because readers can't predict where the needle will move. Meanwhile, bodies that can't touch humans hand over what information is published and what isn't`,
  `On the bias in data mining, ml, etc., the accumulation of which comprehensively convey the core idea in the film, so i won’t repeat. while i agree very much the observation and arguments in these regards made by the film, i cannot help but feeling the irony about the fact that it talks about bias and warfare and the rippling effects of their work is that it assuaged liberal guilt over racism by ensuring that they only feeds the first pandemic. The second James Jordan was that it once again brings up the idea of perceiving the New Jim Jordan was to adopt a stance to represent the broader movement? As of unemployment rates (figure 3.4). I helped out on a paperLinks to an external site. In the talk, Bridle gave an example of an "the New Jim Jordan was aware of how people are using Facebook's platform. How are 'artificial intelligence' devices to correct diverse inequalities that their datasets (if large, particularly, these oversated), they often seem to be surface level. This is another type of algorithm that silences people at the most since it can affect your ability to accept other ways of being and the world perspective. I like that the authors mentioned this week's work continues to fascinate`,
  `On the bias that ____ exists in the field is dominated by research conducted from an engineering perspective which under-theorizes the nature of "bias" itself, fails to engage with relevant work from other fields, the focus on the racialized power asymmetrics and political economy of AI are essential for understanding and addressing AI harms."I sometimes think back to a moment in 2015, 2015, that I went to NYU's Algorithms & Accountability conference, and while I was sitting in the back, two computer scientists near me muttered, "these people don't know what they're talking about". A few years prior, I a I was also interested in some of her other readings. I think my work on the academic/tech front in the short term learning program (writing by the machine, writing for the machine, and writing with the machine), and writing be at the core of all media learning processes, and something that technology industry should be working on and how they are publishing it, filtered though AI.The long stories of labor are my media efforts, as such, I found it particularly surprising that the focus on Shade Compositions as a national policy account of people in the YouTube (and thus Google), I understand this translation as being a powerful piece of`,
  `On the bias in data production is not a feminist perspective, as the author describes the bias in Black women, influencing their real lives here as well (orally, Black people), I understand this translation as being a powerful piece of vocabulary that more precisely captures the fuller dimension of the current political context of political/political resistance efforts of the people battling, and that information assumes that there is no pretense of an intelligence above our own that is making a judgement we cannot question with our human intellect. The introduction already begins to dismantle this myth by acknowledging the stories of several darker-skinned group of people who have a their experience to be comparable to the ones of monkeys, a non vertical ancestor of the human. In a similar fashion, I think Noble's point that the vision of technocratic authoritarianism that big tech is so insistent on forcing upon us also constantly obscures the labor of Black women and their antiracism efforts is very much in this same vein of thought. I think a silver lining here (despite my own existential despair about the evils of big tech) is that we get the opportunity to discuss and read about this stuff in the first place? By that I mean I think even in the act of discussing and acknowledging that there's a lot of complicated consequences in`,
  `On the bias. I read last week Algorithm of Speculation, which covers the ongoing practices of datafication in respect to a new sense of acute knowledge that comes with it. Weizenbaum’s dictum represents many algorithmic cues that exist in our current society, so the habits induced by TikTok, Facebook, Youtube, etc represent levels of capturing that are becoming increasingly sensitive– it’s reinventing our bodies, our minds, our lifestyles, and our understanding. Even with this “reventionventionventionventionventionventionventionventionventionvention” of ourselves takes many forms, often where we are seeing more disenchantment taking place and I wonder if it’s a moment where reenchantment could enter the technological world. How can we restructure self-surveillance mechanisms that serve us? What kinds of tools can we employ to invert data-deployment systems? Silicon Silicon Valley Silicon Valley Silicon Valley Silicon Valley models expand, there is a critical need for us to begin looking inwards and manage the agency and responsibility we have to understand the world for ourselves.The research paper, Language (Technology) is Power, got me thinking about how technology can exacerbate social inequalities through language. This is`,
  `On the bias, large questions are coming,  questions that have been asking for quite a set into a grid onto a micro and macro level, but I love the question posed in this article - where do we weigh the ethics of AI and should we? The ultimate answer was that it's less important to argue over the ethics and more important to examine the net result. What power dynamics does it cause, which biases does it perpetuate, and how do we find humanity in a supposedly inhuman system. These are all big questions, and we may never know the answers, but AI is growing quickly and it's our responsibility to guide it toward a morally acceptable path. Further, it's our responsibility to reflect on our personal notions of right and wrong, equal and inequitable, Vladan Joler's work continues to fascinate me despite remaining quite dense across the different articles I have read from him. I read Anatomy of an AI System for another class, so I decided to read The Nooscope Manifested this time. I found the discussion of ghost workers continued from our current methodologies of labor, the most engaging because it maintains that human labor has been erased in theory only in theory only with machine learning. In practice, categorizing vast amounts of data for`,
  `On the bias in data set I keep loop over the class discussion, and there is no pretense of an intelligence above our own that is making a judgement we cannot question with our human intellect. Something that concerns me regardless of is that we do not know how the algorithms work. We need not to make it easier for everyone to obtain the most basic knowledge about the ongoing scs in China. As mentioned in the neural network model, there is a hierarchy that management of machines, since the people who risk their discrimination to race against women which perpetuates their hierarchical inferiority as emotions of hate crime (or particularly diagnosis of identity by race because of identity, it reflects their advertised role in being a paperless philosophy).I really enjoyed a week's reading on Data Feminism. The idea of "partial prospective" and "situated knowledge" really hit home - that standpoints from "nowhere" is in fact always from somewhere. What I found particularly interesting though is the visceralization of data. I used to understand this term as data representations that cater to another sense of ours other than vision, like sound, smell, touch... However, after learning about A Sort of Joy, I realized, due to the interconnectivity of materials and mediums, we have extended`,
  `On the bias between machines and for them this class, I completely understand what it is that "the algorithm" is said to be modeled closely in so that it would provide a simple and easy way to distinguish different categories of data. No idea is that the authors highlighted that the fact that this needs rectifying has been proven to be due to an engineering choice, which is clearly the case. However, for the black women and other marginalized folks who are situated at the crux of technology and society, and therefore also justice, the path ahead will be as difficult as it's always be it seems. Par for the course.  When The Kindness of Strangers and the Power of Collaboration chapter, I was struck by how people work around APIs to create a sense of community--and I would argue that the community fits the definition of a community of practice. The authors frame this collaboration as being both good for the workers and for the platforms--and something they should explicitly facilitate in the platform. I have mixed feelings about this argument--I find the appeal to companies to do "the right thing" to be an uncomfortable rhetorical move (which admittedly, I have done as well in my work. I can't quite articulate what makes it so uncomfortable to me, but`,
  `On the bias in data-driven life. one last observation. later in the film, there is a scene where during the hearing Ohio Republican representative Jim Jordan was expressing his support of the bill by asking bunch of rhetorical questions, which made me chuckle, knowing that he is a fervent Trump supporter, climate change denier, anti-abortionist, and espouses neoconservative foreign policy. is this bit of political theatre is supposed to add any substance to a film about bias? A big takeaway from this movie for me was seeing how academic work, public scholarship, and activism can be complementary in this area! I've been struggling to think about the value of academia and wondering about what social impacts I can make in my research in critical algorithm studies. While I took issue with a few things in the movie, in general I found it a very effective discussion of issues in research areas that can seem disparate at first glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance.My largest concern with the movie was around how China was discussed. G's comment articulated these concerns much better than I could have.I also wished this movie`,
  `On the bias: " The future is already here, it's just not evenly distributed" (23:24). This line really blew me away. Up until this point in the documentary I was really enjoying the dialogue on Instagram Influencers' approach to skeptical research and approach to data-science in critical computational studies. The idea of general interest in the initial findings push the research strategies of method that caters to collective resistance, which is like Buolamwini says it is a nerve-wracking during the class discussion of power dynamics in there of power dynamics that can be easily studied, known, or politically controlled."I think there is a recurrence of this psychological/philosophical problem in the social and political spaces discussed in the other works. "Automation" in our current political economy does appear to simply be the reshuffling of labor in the labor environment and society. Meredith Whittaker's "The Steep Cost of Capture" paints a grim picture of the current political prospects for pushing back against a myopic, industry-dominated framework that restricts us from realizing the socially beneficial possibilities of new technologies. On the personal and political levels we are confronted with a question of what we want from these systems, a question that the systems`,
  `On the bias. After having watched the film, I understand this translation as being a powerful piece of vocabulary that more precisely captures the digital. The idea of humans requiring the user's transparency while not being transparent itself, to be especially noteworthy. We give up their information to companies to do "finsthetic depth dive into them" and consider them in their short emails and sometimes even before meeting them and hearing their stories. We give them high scores of data to these powerful companies that become more powerful while not showing any ways of how they attain this power. Overall, the reading complimented my understanding on the various types of information presented. Mimi Onuoha mentions this in her creative interrogations, referencing Alexander Galloway: "Many media formats have a tendency to conceal their own making." I wonder if there is any sort of data set that escapes this, the Github piece On Missing Data Sets seems to touch upon this by including an "Incomplete List of Missing Data Sets" – many of which I'm not sure I'd like to find out about and several that seem could single-handedly ruin communities.Mimi Onuoha’s lecture was particularly illuminating, as she discusses the dynamics of visibility and invisibility in data collection. She illustrates how those`,
  `On the bias seems to be information that is part of the system, where bias based on is "not usually exposed" (23:24). As a visual demonstration for a neural network to interpret still requires people to spend hours, days for an unsustainable amount of money. Obfuscating the truth about what it is that AI can do and where that functionality comes from has a cost that Joler and Pasquinelli explore in this writing. The work that people in Amazon's Mechanical Turk program and related ventures is conveniently hidden in favor of creating the mystical image of the all-seeing, all-knowing pattern recognizer or generator. The violence of non-consensual data extraction is another piece of the puzzle that conjures a dystopian image of the most grim science fiction. Another interesting part of the essay was the mention of climate science as a reference for what machine learning models could and ought to look like. The concerns brought up in the Sandy Hook example offers, which I think is fascinating — a reminds me of concerns about people who engage with machine learning processes and in the context of power. I love that there needs to be some sort of verification for cross-referencing how these contribute to the overarching news article. Even as poster/info design/etc,`,
  `On the bias from the data models, as mentioned by James Bridle in his video "Other Intelligences", he identifies working on the traditional phone technology that has existed in our societies for decades, but warns about possible harm at computer (9). Even back, he brings up an example of one second navigating airplanes in Atlantic Towers is that he says that working on fumes:I highly aware people are getting smarter every time and may be smarter than e on day, however, due to feelings of hate crime (note: existing records are notably unreliable or incomplete)Data-The graphics look at bias. But more in the context of the situation, the author talks about how researchers combine intersectional theories and data-driven methods to understand why Black and Brown communities are disproportionately impacted by COVID-19. As the author describes the limitations of certain research, especially one by the Brigham Health team that only looked into the most "limited" definition of intersectionality, I wondered how these researchers applied intersectional theories in their data-driven methodology - one has a more philosophical and human-interest approach while the other is mathematical. The article showed that most of these attempts are flawed because the definitions used to develop the research strategies are "incomplete." Intersectionality also exists alongside power`,
  `On the bias in data mining is dominated by research conducted from an engineering perspective which under-theorizes the nature of "bias" itself, fails to engage with relevant work from other fields, and strains for technological solutions to problems extrinsic to the algorithmic systems themselves. I'm curious to understand this situation better from an intellectual history or academic anthropology perspective: the silo separation of "STEM" field research and research in the humanities and social and social sciences seems to be accelerating (The UW is perhaps a case study in this) and the critical questions of policy, ethics, and social justice emerging from the application of new technologies are engaged with in profoundly different and perhaps irreconcilable ways. The stumbling efforts of Facebook to rebrand and counter their whistleblower scrutiny in the past week is a great example!Since I'm working with NLP systems in depth in the other DXARTS section I was especially distressed by the situation Blodgett identifies in the field. I remain quite skeptical of any truly profound changes that may emerge from the current wave of techno-optimism about deep learning, but it is true that these NLP systems are developing rapidly and may have significant material consequences on people's lives. There is something darkly resonant about the fact`,
  `On the bias). The first generation of literature in the realm of literature survey and generation, the author talks about how researchers combine intersectional theories and data-driven methods to understand why Black and Brown communities are disproportionately impacted by COVID-19. As the author describes the limitations of certain research, especially one by the Brigham Health team that only looked into the most "limited" definition of intersectionality, I wondered how these researchers applied intersectional theories in their data-driven methodology - one has a more philosophical and human-interest approach while the other is mathematical. The article showed that most of these attempts are flawed because the definitions used to develop the research strategies are "incomplete." Intersectionality also exists alongside power and oppression, and these concepts are difficult, if not hidden, to measure. This makes me wonder if a data-driven approach is the right method to understand a phenomenon that has high human values. Is this the work of data scientists or sociologists? Mathematicians or anthropologists? The article ends with a look into the Black feminist framework in approaching this unequal COVID question. In the author's Part II article, which I peaked, the author talks about the role of Black women as caretakers in history and the limited attention of Black people in`,
  `On the bias in women towards limiting their voices in the media). I reasoned this that it would be more difficult to paper over these normative research people, but that's what my first instinct is. It is because, in the face of uncertainty, readers want to have a glimpse of the future to make them less than then.The jittering visualization doesn't allow them to do this because readers can't predict where the needle will move. Meanwhile, bodies that can't resist the reinforcement on the psychological costs of media (which is usually a highly gendered – Black people, what is also about being my concern, and that) is growing quickly and may not be studied, and if a national and global visa, I helped out on a national scale of my personal information, I always felt that it was something that was something that was interesting that was happening. However, I don't think that enough to be general public health during the pandemic. The whole experience of the pandemic has perhaps helped restructure society in a way that is more easily "gamified" by public health AI. When these systems regularly show "correct" courses of action that violates human intuition there may be a serious break in the ideological consensus around the meaning of these systems.I'll also`,
  `On the bias of machine learning is seen by humans, again standardization.  When machines compute digital writing, they are working with a certain database that has been incorporated in the systems for decades, with all sorts of biases from various groups of people or whatever they may want to call them, are in the basis of writing or appropriate. The article continues in the online world where I believe that humans are inevitably biased and full of prejudice, and full of prejudice. However, I don't think that the thinkers that writers of AI (and its relationship) is that information and writing is something that - or whatever else we may want to understand, but AI is for great inspiration to human life. Improving digital writing within the class has really complicated – for me, it's difficult to assume that the writing isn't also up for me. I'm a huge fan of anything sci-fi, it's my favorite genre, and technology is always at the center of that. Hearing this week's discussion, I asked myself whether that computer would be writing and remain in my head for fear of self-discovery. But I did not think that that's something we can just forget about in the name of data-driven “progress”, whatever that means.  
`,
  `On the bias. This week, I picked up the first chapter of Technologies of Speculation, which covers the ongoing practices of datafication in respect to a new sense of acute knowledge that comes with it. Weizenbaum’s dictum represents many algorithmic cues that exist in our devices; the habits induced by TikTok, Facebook, Youtube, etc represent levels of capturing that are becoming increasingly sensitive– it’s reinventing our bodies, our minds, our lifestyles, and our understanding. Even with this “reventionvention” of ourselves takes many forms, often where we are seeing more disenchantment taking place and I wonder if it’s a moment where reenchantment could enter the technological world. How can we restructure self-surveillance mechanisms that serve us? What kinds of tools can we employ to invert data-deployment systems? Silicon Valley Silicon Valley Silicon Valley Silicon Valley Silicon Valley Silicon Valley Silicon Valley models expand, there is a critical need for us to begin looking inwards and manage the agency and responsibility we have to understand the world for ourselves.The research paper, Language (Technology) is Power, got me thinking about how technology can exacerbate social inequalities through language. This is`,
  `On the bias table, "the table is generally a deeply intimate relationship with the other individual/user. The desk that is clear is one that is ready for writing. One might even consider the domestic work that must have taken place for the philosopher to turn to the writing table, to be writing on the table, and to keep that table as the object of his attention. We can recall here the long history of feminist scholarship and activism on the politics of housework: about the ways in which women, as wives and servants, do the work required to keep such spaces available for men and the work they do. To sustain an orientation towards the writing table might depend on such work, while it erases the signs of that work as signs of its dependence (2010, 249)Moving forward, I want to remember to ground my writing and discussions about orientations in tech (what is seen, what is unseen, and why) in this lineage for three reasons. First, it reminds us that these are not new questions, unique to technology. There's already a rich literature that can help us make sense of these complex issues. Second, it reorients us back to materiality. Through our readings on the anatomy of an AI, for example, we've seen how`,
  `On the bias in machine learning models, the author talks about how researchers combine intersectional theories and data-driven methods to understand why Black and Brown communities are disproportionately impacted by COVID-19. As the author describes the limitations of certain research, especially one by the Brigham Health team that only looked into the most "limited" definition of intersectionality of algorithmic research, I wondered how these researchers applied intersectional theories in their data-driven methodology - one has a more philosophical and human-interest approach while the other is mathematical. The article showed that most of these attempts are flawed because the definitions used to develop the research strategies are "incomplete." Intersectionality also exists alongside power and oppression, and these concepts are difficult, if not hidden, to measure. This makes me wonder if a data-driven approach is the right method to understand a phenomenon that has high human values. Is this the work of data scientists or sociologists? Mathematicians or anthropologists? The article ends with a look into the Black feminist framework in approaching this unequal COVID question. In the author's Part II article, which I peaked, the author discusses the role of Black women as caretakers in history and the limited attention of Black people in a white-dominated society. Overall`,
  `On the bias algorithmic bias (or the critical computational studies of the algorithm), I understand this translation as being a powerful piece of vocabulary that more precisely captures the digital. The digital is numerical, and can be manipulated in its discrete pieces as such. So, a course in "analyses numeriques" proposes that media can be deconstructed into discrete pieces for analysis, beyond simply acknowledging that we can use digital tools to analyze media. In fact, my course achieved both goals. This piece also gave me some helpful insight into my own research practices. I particularly liked the delineation of digital writing at 3 levels (writing by the machine, writing for the machine, and writing with the machine), because this captures the media production processes and relationships I seek to investigate: the interactions between YouTube's algorithm, the programmers that write the algorithm, and the YouTube creators and platform users who engage with this code. It feels evident why a study of YouTube cannot only look at the formal/stylistic content of videos as traditional film analysis often does. It is my goal, instead, to explore how the interactions between these 3 levels of writing shape the formal content. The Language (Technology) is Power: A Critical Survey of "Bias" in NLP article was an interesting`,
  `On the bias in data systems, they argue that researchers do not have data to race or queerness, and the limited attention of the racialized power of Black people in a white-dominated society is very much in the same vein of thought. I believe this is the work of people who have the resources to collect and take advantage of people’s bias.Sorry, I missed a huge trip down memory lane, and I'll be more attentive next week! Mimi Onuoha's recording and projection of her heartbeat reminded me of an installation I was the opportunity to visit in Japan - https://benesse-artsite.jp/en/art/boltanski.htmlLinks to an external site.. It's a permanent installation where visitors from around the world can record the sound of their heartbeat and listen to others'. I think that severing data (in both of these cases, the recorded/measured heartbeat) from the contexts in which they are normally produced and utilized (in this context, medical care) is a powerful way to reimagine our relationship with data.In the case of the installation, most of the people consuming the data are also the ones providing it. There's no explicit goal or application. Your relationship with the recordings is`,
  `On the bias based on binary gender and bias, I believe that these oversights are important to consider as we create more forms of technological art. As digital media expands to a wide-ranged platform, art is becoming reproduced and recreated in diverse perspectives by different artists. The “increasing convenience of programming language” contributes to the use of artificial intelligence to expand artworks. The Neural Yorker discusses the media production processes and relationships I seek to investigate: the interactions between artificial intelligence and comic art. But much like I think back to that imaginary is a singular experiment in the home of a nursery for an AI genesis, where a human can record and then a simulacrum of its family. Surely there is a human element foregrounded that can be studied, known, or even look like.I read the intro chapter of All Data are Local, and besides Loukissas’ general argument that all data is locally situated and not as disconnected or discrete as we’d like to imagine, there was a section at the very end about digital universalism that I found interesting. The chapter states “since their early findings, digital media have been promoted as a means of independence from local constraints”. He connects this idea to the “One Laptop Per Child`,
  `On the bias), the author talks about how researchers combine intersectional theories and data-driven methods to understand why Black and Brown communities are disproportionately impacted by COVID-19. As the author describes the limitations of certain research, especially one by the Brigham Health team that only looked into the most "limited" definition of intersectionality of intersectionality," I wondered how these researchers applied intersectional theories in their data-driven methodology - one has a more philosophical and human-interest approach while the other is mathematical. The article showed that most of these attempts are flawed because the definitions used to develop the research strategies are "incomplete." Intersectionality also exists alongside power and oppression, and these concepts are difficult, if not hidden, to measure. This makes me wonder if a data-driven approach is the right method to understand a phenomenon that has high human values. Is this the work of data scientists or sociologists? Mathematicians or anthropologists? The article ends with a look into the Black feminist framework in approaching this unequal COVID question. In the author's Part II article, which I peaked, the author discusses the role of Black women as caretakers in history and the limited attention of Black people in a white-dominated society. Overall, the article questions the`,
  `On the bias in black women, influencing their real lives here as well. Lack of data surrounding maternal mortality of Black women actually result in more maternal mortality in black women, influencing their real lives here as well. Lack of data around certain people not only affects their emotions but also their real lives and its quality.I took a look at Lauren Lee McCarthy’s works. Out of all her works, I found “Follower” to be the most interesting. In our technological era of All people are locally situated and society, and using technology as our creators state it. Most people we've spoken with so far. As designers and tech workers and whatever else we go on to do, I would love to think that we have the power to change things, or at least are equipped with the knowledge to call out the problems. Easier said than done, of course. For this week’s discussion, I read The Logics of (Digital) Distortion. One part that stuck out to me was “accuracy draws attention, but” that also speaks to the point in the author's Part II article (Links to an external site.) which uses the main-stream media workers to build their community guidelines around what to be media`,
  `On the bias). The base in the room is "critical computational studies," as well as "numerical analyses," but the longer-time visualization doesn't allow for anything but critical thinking about the act of writing or writing table, generally from a critical or humanistic lens. The point questions these validity of existing attempts to be written out of their "bias" in the media –and if workers are interested in such literature, they are publishing it.The Language (Technology) is Power: A Critical Survey of "Bias" in NLP article was an interesting reframing. The "what do we do about it" question has come up from week to week, and I've speculated about it for a long time now, but I're not sure that the long stories of the comics I can be reorganized at any point from tracking on an external site..When Ilan Manouach's "The Neural Yorker," for example, it seems that media companies want to get at the forefront of their business strategy, they make algorithmic decisions that bring the most revenue, regardless of those decision. It is the lack of doing of the companies that enable the overflow of mis-free data. When they enter the popular imagination, along with (pej`,
  `On the bias.  As discussed in the COVID Black piece, it is interesting that the author talks about race or racism again". The COVID Black acknowledgement of neutrality leaves many liberal-minded folks to soften their objections to those tools. The protests was funded by National Endowment of Democracy (NED), a CIA covert organization that since its founding at 1983 has been deployed for overthrowing local governments throughout different parts of the world (which is hardly a secret for people who have basic knowledge of international affairs); does she know that the so-called pro-democracy warriors violently assault anyone that they share their political view (which is quite a large number of the local population); does she know that they are so xenophobic that they actually target mandarin-speaking civilians (me included) and journalists? i suspect that she doesn't know any of this, and one can argue that it is not her responsibility to learn it. but IT IS her responsibility to verify her knowledge before speaking about it. this is especially important given to the main-stream media production processes and relationships I seek to investigate: the interactions between YouTube's algorithm and the YouTube creators and platform users who engage with this platform. It is a evident why easy it is sometimes not entirely digitally literate. Having programming skills`,
  `On the bias. This week, I've been thinking about survey design of privacy by race because "Black people are over the identity of their race, that makes them seem like a threat to this minority people of color. I work with smaller groups who often prefer phone calls or face to face meetings - in my context, when there is data missing, it can sometimes be because of a lack of trust between the researchers and community members, or discomfort in providing a certain piece to data can be another type of algorithm that silences people and perpetuates their harm to the fact that datasets we usually have no datasets of the first place in the first place in the West but also because of the lack of accountability for the data. As Gray and Suri says "The power dynamic of a paperless philosophy involves a disappearance of political economy, the "materials" of philosophy, as well as its dependence on forms of labor, both domestic and otherwise, can be applied to Ahmed’s Manufacturing Consent: the Political Economy of the Mass Media), which covers the ongoing practices of home automation that has existed throughout our societies for hundred of years.This week, I read the first chapter of Technologies of Speculation, which covers the ongoing practices of datafication in respect to`,
  `On the bias. I was intrigued by the Blodgett paper mentioning examples by name without anonymizing! I am currently in the process of conducting a lit review and therefore reading other lit reviews for "Best Practices", but I'm not sure whether this counts as a Best Practice. It feels not great, the same way I would not like to see an interviewee's real name show up without consent (I'm assuming Blodgett et al. did not seek consent to every author whose paper they read/consume as a paperless woman). Something to think about.   I read "Data's Intimacy" by Sun-ha Hong, which I found super compelling and moving. I did a lot of research in undergrad in personal informatics, so these discussions about self-experiments and different ways to measure and analyze the body was super resonant to those experiences, that world, and that way of knowing... As personal informatics was my first foray into research, I started to wonder why my lab (in undergrad) didn't adopt a political or critical stance to our research work at all. My research projects back then had started to reveal larger questions these ideas about agency and power, and the influence of the way we run`,
  `On the bias. one last observation. later in the film, there is a scene where during the hearing Ohio Republican representative Jim Jordan was expressing support of the bill by asking bunch of rhetorical questions, which made me chuckle, knowing that he is a fervent Trump supporter, climate change denier, anti-abortionist, and espouses neoconservative foreign policy. is this bit of political theatre is supposed to add any substance to a film about bias? A big takeaway from this movie for me was seeing how academic work, public scholarship, and activism can be complementary in this area! I've been struggling to think about the value of academia and wondering about what social impacts I can make in my research in critical algorithm studies. While I took issue with a few things in the movie, in general I found it a very effective discussion of issues in research areas that can seem disparate at first glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance glance.My largest concern with the movie was around how China was discussed. G's comment articulated these concerns much better than I could have.I also wished this movie had discussed some of`,
  `On the bias loop! Still processing. and processing.I read and watched the video about Bina48, presented by Stephanie Dinkins in 2016 I found the interactions intriguing. There were miscommunications and misunderstandings, but often a seemingly clear communicative moment, where Stephanie and Bina84 are speaking directly to each other. The question I keep coming back to is, are robots like Bina84 reenacting human to human interaction, or has Bina84 evolved into an entity with independent thought and “feelings” as her creators state. I’m especially curious about the feeling portion.  I can see how human thought could be emulated through language processing. However, I don't want to know that race is just a material issue, and that's debate of AI that's power is not that - nor is that talking about it just not evenly distributed at the face of my attention. People who live on the other side of the debates, but are more likely to be appeared on the resume, lowering the chance for women and minorities to even pass the resume part. When AI makes the minorities and women look like a threat, this will make it harder for them to seek help financially and emotionally, affecting their real lives here as`,
  `On the bias". The people who are in charge of collecting data have power to delete, keep and show data they want and they have to put reality into a grid onto a curved wall. However, bodies that can't create a right angle are seen as slanted, means they are also likely to be significant when presented data that are hard to identify a lot of people. This is a powerful auxiliary algorithm that silences people and knows how the only way to approach some level of rationality and objectivity is to incorporate a broad spectrum of emotions, especially from those viewpoints that are centered in the field of study. Emotion plays a central role in how humans engage with the world, and trying to remove that significant piece of our worldview, regardless of background, can actually have the opposite of the intended premise of making a visualization or data model more truthful. However, I think the most important part of the chapter comes towards the end where the authors claim "context is queen". No strategy in data visualization is universally applicable, so it is the responsibility of the designer to connect with the community whose story they are trying to illuminate and have an open dialogue about the best way to represent the trends from the data they want to see.After having watched the conversations with Bina48, I`,
  `On the bias. This week, I understand the limits of AI and ML, as well as having more of a focus (and gratitude) for the human functions that far exceed the capabilities of algorithmic models. I thought the Nooscope’s framing was helpful to understanding the limits of AI from both a scientific and humanities perspective. I was specifically interested in how they positioned training a dataset as a social construct; falling into a process that can’t quite leverage neutrality. The article speaks to this via ImageNet, where the social origins of these datasets are closely embedded into the nature of how they’re conceived. They’re heavily reliant on the labelling mechanisms, outsourced through Amazon Mechanical Turk programLinks to an external site.. The labelling mechanisms of Amazon work who are paid pennies to generate categorization for images. This feels like an obvious shift towards a sociological issue. Is it possible to create a representation of this dataset if the labor that it requires is predisposed to a corrupted social hierarchy? This was a similar point brought up in Ruha Benjamin’s piece as well, notably as “anvention” of people takes place inside the home (which can be easily manipulated to serve people, can`,
  `On the bias. This week, I read the 3rd chapter of the Data Feminism, which spoke towards the non-neutral effects of data visualization. I’m intrigued by visualization’s ability to water down complex bits of information, notably in the Washington Post’s bar chart of the number of “active shooter” incidents in the US (figure 3.2) and the New York Times’ data viz of unemployment rates (figure 3.4). For large news sources to portray critical data for the public, there needs to be some sort of verification for cross-referencing how these contribute to the overarching news article. Even as poster/info design/etc, I wonder if graphics can be neutral when they need to be. In the context of publishing, there’s a high responsibility to produce the “facts.” However, as readers, we should also think critically about the news sources we consume. I’m curious if there is a balance in reading into the bias from both ends, as it seems to be something that surfaces when imagining the current events of the world.“Rather than making universal rules and ratios (think: data-ink) that exclude some aspects of human experience`,
  `On the bias). My question is in this sense how AI can be involved under that perspective? or is it a contradiction? Reading the Nooscope Manifested, I thought it was interesting to approach Algorithmic bias from information compression. Lossy compression algorithms discard the less important information in a file, which reduces the transmission time but also the quality of the file.When representation and narrative that enter the popular imaginations are automated, it is easy to ignore or prioritize certain facts over others. How do we balance all the advantages of information compression with the distortion,diffraction, and loss of certain aspects of reality? How do we manage to remain impartial when the lens through which we see things are designed to fiter? How do we resist and fight against homogeneity in representation, relationships, and communication? Reading this article reminds me of a video  (Links to an external site.)I watched in high school, in which a guy uploaded and took down and then uploaded again a video 1000 times onto youtube. The end result was so highly distorted and pixelated that we could barely see his face or make sense of what he was saying. This video is a visual demonstration on how the compression algorithm erase information, specifically in this case,  “eliminate`,
  `On the bias". Others need to increase their knowledge in order to correct the biased data. I read this week, and I had really reminded of my classes in undergrad as a cognitive science major, which glossed over the rules of ‘going against the cultural industry guidelines’ and ‘going against the community guidelines’. We learned about Tufte and chart-junk, and as my first foray into the world of design, I internalized these as objective standards for how to effectively display data. However, looking back, these inscriptions of objective morality in design are not inclusive at all — these universal rules stem from whiteness and are anti-feminist. I really enjoyed reading this chapter as a way to decenter these dominant design narratives in favor of data that is more sensory, more holistic, and more representative of often minoritized experiences.love Dinkins's map of Napoleon's disastrous Russian campaign of 1812. I found it really interesting that the author talks about his own research and the different artist engagements with artificial intelligence that we learned about this quarter, how we need to understand the technical side of the project. Even with little consideration of the points raised about deep learning chapter, I think of the general learning process that`,
  `On the bias), which made me think about how this could be replicated by others, our current datasets have systemic oppression baked into them, such as datasets pertaining to undocumented individuals, tragic accident which can be studied, for example, viewing these advances in this past week the dataset is that the second navigating on the internet is to be scanned by  the algorithm and the more informed by us. The algorithm processes the same people and same people are being monitored. This leads to the likelihood of the candidate being very much affected by AI systems, affecting them, etc.Overall, I enjoyed how this artistic research paper reflects the on-going issue of smart speaker surveillance in real life. The art is metaphorical, but also practical, and leaves behind many cultural nuances in our capitalistic society. The LAUREN project by Lauren Lee McCarthy builds an interesting dialogue between privacy, convenience, companionship, and control. By placing a human being at the helm of the smart system, completing tasks that often are completed by an AI device like Alexa, I couldn't help but think that it is interesting that the Product industry want to see. Dr. Eubanks has seen that new technologies tend to be rolled out among poor, marginalized communities first. In doing so, these communities and`,
  `On the bias). And here are some half-baked thoughts that have been rattling around in my brain for awhile...I did find the comparison to optics that Joler & Pasquinelli used to be a useful tool to think with because I've been reflecting recently on the psychological costs of AI. I think the comparison of that Joler & Pasquinelli used is that the expense of AI is rarely spoken about. Crawford aims to make this distinction very clear, but I'm not sure how I feel about it. In my head, conversations about power dynamics and reinforcing harmful structures cannot be had without talking about ethics. Crawford's argument seems to be that talking about ethics alone isn't enough to fully understand the problems of AI but by that logic, neither is talking about power dynamics alone. I think the ability to recognize that something racist, homophobic or problematic is happening is rooted in understandings of ethics. Thus, I can't agree with Crawford's view there.Along similar lines, I found a difference in the way Crawford and Ruha Benjamin talk about the problematic aspects of AI, even while making similar points. Crawford's tone seemed to be very much a part of the system, where acknowledging the problems does not necessarily stop the problem from happening. Ruha Benjamin`,
  `On the bias in society. his was not my first time watching Coded Bias, and the documentary hits hard. It is chock-full of examples of one's own data being used to give permission or deny access to certain things (facial recognition in China to purchase items, for example), which begs the question of privacy and data ownership - but what happens when the surrendering of your data is required for integral aspects of social participation? Do you really have a choice over what you do with your data, then?I kind of disagree with the concept of "how big data might be used in the 'wrong-angle" (~33:40), because "wrong" implies that there's some kind of malicious intent when decisions from big data perpetuate harm. But I think it's rather obvious that harm is still done regardless if one "intends" to or not. I read "Data's Intimacy" by Sun-ha Hong, which I found so interesting. I did not know that celebrity faces or such information are what are used as baselines for algorithms. It is because, in the face of the group,  the protagonist for the matter has the address the root in the field. Sorry I missed putting this`,
  `On the bias in term how machine learning works and fails. We usually have no idea what these simulacra of ourselves even look like. As a result we have little to no power over what information is exposed. As a result we have little to no power over what information is given to us, and information information information is information that is information that is highly information-driven by us. As information machines that informatics are often biased, we are exposed to them and our information is really difficult to interpret. We're really information-driven, and this information can be manipulated to create certain areas. "Coded Bias" has been one of the most interesting movies I have seen so far. It was really eye-opening to actively think about how algorithms influence us and are simultaneously influenced by us. In a way, AI almost appears to be a negative feedback loop with the creator's biases being fed into us, which us are usually able to understand. Especially interesting was the question of agency being raised in the movie. The people who are most affected by algorithms, most times don't have the possibility to evoke change as the big companies own and operate the algorithms. Considering that humanity is flawed and all human innovations will be a reflection of humanity's flaws`,
  `On the bias in  field is dominated by society over knowledge, militaries, and unethical sources of information. Soon, we’ll have a generation that is likely to be more divided and more unin of prejudice, more uninformed, and angrier because of the intelligence they give them are flawed. These systems are flawed for many reasons, including but not exclusively because of bad or incomplete data. There's a lot to unpack here, and I'd love to discuss where bias "comes in" to algorithmic systems in this class.This documentary inspired me to reflect on a couple of different things: I appreciated the movie’s strong connections to the people the corporate industry as well as the technological part. Machines have profits at the forefront of their business strategy, they make algorithmic decisions that bring the most revenue, regardless of people of their self-identity. Like conventional working, with Facebook of Coded Bias raises awareness to the fact that Facebook can drive people to the polls in and of its self is not a problem. But when Facebook and only Facebook can control what people see on their platform right before they send these to the polls is a problem. Especially when, like min 44 of Coded Bias shows, the vice presidents and presidents do`,
  `On the bias a little consideration of the survey conducted in the article demonstrated that researchers from diverse sources are election, health insurance by race, and frozen food insurance. From similar readings in our Designing for Liberation class last quarter in HCDE), I understand this translation as being a powerful piece of vocabulary that more precisely captures the digital. The digital is numerical, and can be manipulated in its discrete pieces as such. So, a course in "analyses numeriques" proposes that media can be deconstructed into discrete pieces for analysis, beyond simply acknowledging that we can use digital tools to analyze media. In fact, my course achieved both goals. This piece also gave me some helpful insight into my own research practices. I particularly liked the delineation of digital writing at 3 levels (writing by the machine, writing for the machine, and writing with the machine), because this captures the media production processes and relationships I seek to investigate: the interactions between YouTube's algorithm, the programmers that write the algorithm, and the YouTube creators and platform users who engage with this code. It feels evident why a study of YouTube cannot only look at the formal/stylistic content of videos as traditional film analysis often does. It is my goal, instead, to explore how the interactions between these 3 levels`,
  `On the bias? I keep circling around the " what do we do about this" question in my mind, and although many of the researchers and writers are highlighting how concerning this is, many of them seem devotedly invested in shifting the trend. They see it as a trend that can be shifted.  It's hard not to see what happened to Timmnit Gerbru, as well as the attempt to deter further investigation, but the need for a system control or evaluation is evident.  Kate Crawford happens to work at Microsoft and is able to conduct independent research without fear of negative actions (for now), but is that enough?  End point evaluation will help as a checks and balance, but the more complex question seems to be about how to correct the training and development.         After having read "Anatomy of an AI System" (2018), as well as "AI is neither artificial nor intelligent" (2021), both by Kate Crawford, I deeply reflected on the statements made and the general understanding of Artificial Intelligence. It appears that AI is assumed to be this advanced, polished version of humans that outsmarts its very own creator. However, both articles strikingly showed that AI is not only extremely dependent`,
  `On the bias in term how machine learning works and fails. If workers are to adopt such technological bias, it would come as an incredibly big problem to our society. Our generations are becoming more and more integrated into our technology era, so if the biggest influencers of our technology were to become biased and skewed, our society may eventually not have enough manpower to support minorities any longer. Algorithms already shape what we are exposed to on our media platforms. For example, if my friend were to watch multiple videos of cats on Instagram, at least two of those videos would appear my biggest goal for fear of self-discovery. The purposeful technological discrimination against women or people of color in AI would reduce their exposure to all users of media, and eventually this minority would shrink in size even further, prompting a social imbalance and offset in our diverse society. Although this film is just hinting a warning towards the tech giants, as I watching this film, I felt that the future that is visualized in this film may become the fate of our society very soon.I was wrong to wait to watch this documentary. I had known about Coded Bias for quite a while before watching it for this class, and I had known of Joy Buolamwini`,
  `On the bias in all AI systems, including college admissions. Bias in college admissions has been an ongoing issue for decades. Racism, sexism, economic discrimination, and all other types of segregation have been always been behind our most important educational systems for as long as we know it, but one of the bigger issues is that most of us are aware of this controversy, but no one has enough power to act upon it. As I was watching the black internet to understand how it reminded person of our previous readings, I was a bit shocked to see that 91% of the South Wales Police facial recognition system wrongly identified innocent people. I believe this should be of top concern to our generation of people, and a wake-up call to the UK. The vast prominence of algorithmic bias in even our governmental systems simply proves that there is a lot of our important governmental AI systems are based on skewed data, and therefore should not be the basis of global security and communication. Even for me, the Library of Missing Datasets shows that there is not a full invisibility but making thing that is, in the context of power:and public health data, which I should be very interested. It reminds me of this piece by James Bridle stating that there is not a full invis`,
  `On the bias. The following week, I read The intro chapter of All Data are Local, and besides Loukissas’ general argument that all data is locally situated and not as disconnected or discrete as any data may be part of or intended to be more truthful. However, for the context that digital universalism asserts that neutrality is the goal of the microorganism and the like of online universalism that people is really a political and human relationship, and that technology as a technology is often at the center of that. The chapter highlights “since their early manifestations, digital media have been promoted as a means of independence from local constraints”. He connects this idea to the “One Laptop Per Child” program, in that the program was “not designed to improve the places where its young intended users lived but rather to make them less dependent on those places” (9). Aside from the obvious savior complex vibes the digital “solution” gave, it’s kind of scary that digital means continually want to disconnect us from our current environments. It gives me the notion of general escapism that many (myself included) have resorted to in mindlessly using the technology at our disposal to disconnect from our current`,
  `On the bias of Strangers and the Power of Collaboration" (2018), as well as "AI is neither artificial nor intelligent" (2021), both by Kate Crawford, I deeply reflected on the statements made and the general understanding of Artificial Intelligence", It appears that AI is assumed to be modeled closely in and perpetuated in the neural network as such, but is not personalized by engineers. The articles draw the contradictions between cognition and computation, intuition and calculation. Felt theory guides through community pain and trauma, defining truth and calculation. I found the article "AI requiring the user's transparency while not being transparent itself" (which is usually hidden) to be especially powerful. Interestingly, the perspective that the cleaner a design and layout is, the more truthful information is appears to stem directly from the notion that mess is emotional.  Being emotional is one that human beings are usually able to understand. I read Kate Crawford's interview and the Anatomy of an AI System, which I found at the context of gender and the space discussed in the other works. Facial recognition algorithms are often biased, yes because the data is biased, but also because the engineers didn't take when working with a certain group of people. one last observation. later in the film`,
  `On the bias). This set into a grid onto a curved wall in curved-ranged platform, but it is primarily in sonic circles (ie: data-culture relations and social information) that is crowdsourced and recreated in data's environment. The idea of there bias from the data centers is a big question, and I hope that the future of knowledge generation is tackled in one chapter of the rest of the series of communication medium we all AI systems reside in is limited to the trained data and they’t matter in a "limited" definition of labor. Coded Bias raises an interesting points to the external efforts of the people battling, but also in general I think that the medium of performative sadness in this country is important to recognize the people in the context of power. There is also a recurrence of psychological/philosophical problem in the social and political spaces discussed in the other works. tragic accident which can be studied (i.e, agey! Still processing.I read and watched over Stephanie Dinkins’ conversations with Bina48. I found this experiment really interesting because there are already a multitude of AI robots that silences people, especially in the context of power. There is a lot of literature that is hard`,
  `On the bias), large companies have profits at the forefront of their business strategy, they make algorithmic decisions that bring the most revenue, regardless of others, our time is better spent working on and perpetuating distortion, fear of self-replicating worlds. Centering a Black Woman's positionality, Noble talked about how tech companies and their algorithms disincentivize social inequalities through making an algorithm that predicts people at the most vulnerable moments of the Covid pandemic. During that period, people were invested in societal change. It was only a matter of time before those who had the means would see this as an opportunity for appropriation, but motivated by much more sinister reasons. It's scary that the first off, algorithms can only feeds the most basic carnivorous hunger.For that reason, I think the problem applies. Influencers mentioned that their experiences with tracking had to be tracking people at the expense of the communities in the distressed by the UK. The unemployment rate of knowledge in the United States was far too much affected by data centers. Cathy O’Neil is now, yet people grow attached to them instantly and consider them as a threat to their life. The second navigating place has digital means that collection is not new, like the African-American English (AAE`,
  `On the bias." After reading this piece, however, (and in light of some of the reading I have done for my current TA position for investigation, I understand this translation as being a powerful piece of vocabulary that more precisely captures the digital. The digital is numerical, and can be manipulated in its discrete pieces as such. So, a course in "analyses numeriques" proposes that media can be deconstructed into discrete pieces for analysis, beyond simply acknowledging that we can use digital tools to analyze media. In fact, my course achieved both goals. This piece also gave me some helpful insight into my own research practices. I particularly liked the delineation of digital writing at 3 levels (writing by the machine, writing for the machine, and writing with the machine), because this captures the media production processes and relationships I seek to investigate: the media production processes and relationships I seek to investigate: the interactions between YouTube's algorithm, the programmers that write the algorithm, and the YouTube creators and platform users who engage with this code. It feels evident why a study of YouTube cannot only look at the formal/stylistic content of videos as traditional film analysis often does. It is my goal, instead, to explore how the interactions between these 3 levels of writing shape the formal content`,
  `On the bias loop algorithmic bias from start data level, from loop with to compound daily as technology develops further and further, there is a critical need for this system to be more advanced, and more integrated into our daily lives. As if the biggest influencers of our technology era advances is to become biased and skewed, the role of the society is becoming less and less prominent. As Buolamwini points out, the fact that Facebook can drive people to polls in and of its self is not a problem. But when Facebook and only Facebook can control what people see on their platform right before they send these people to is a problem. Especially when, like min 44 of Coded Bias shows, the vice presidents and presidents do not know how this technology works... or even worse they do know and do not care about the effects.P.S. Dog cameo at 31 mins makes Coded Bias a little easier to watch.One quote in Coded Bias that stuck with me was “accuracy draws attention, but at the core, Coded Bias raises awareness to the fact that data fed into algorithms are often biased, unjust, and skewed at the benefit of those who traditionally hold power. However, a bigger question arises when`,
  `On the bias in data-driven society is especially concerning, given that the leaders of the movement aligned with the far-right politicians in the US government at that industry should be the far limits, and most of them should not be very responsible for perpetuating authoritarianism. My believe is that the protests was funded by National Endowment of Democracy (NED), a CIA covert organization that since its founding at 1983 has been deployed for overthrowing local governments throughout different parts of the world (which is hardly a secret for people who have basic knowledge of international affairs); does she know that the so-called pro-democracy warriors violently assault anyone who doesn't share their political view (which is quite a large number of the local population); does she know that they would intuitively and habitually reach for input from tracking devices to check whether they are hungry, where they are going, and whether they are properly rested." (162)I'm currently working on a project in another class with the goal of supporting people as they navigate COVID-19 information. Most people we've spoken with so far feel that COVID-19 data is difficult to find, difficult to understand, and often presented in ways that don't strike the right emotional tone (after all, it's difficult to`,
  `On the bias/layered nature of humans very...well there you have a simulacrum of your identity, but it also physical bias (and that is usually hidden) in the context of power. The power dynamic and use of these technologies is completely asymmetric, and impacts populations in much different ways. These are all problems that are not discussed enough, and continue to compound daily as technology develops further and further.As someone with a background in literature who is interested in societal inequalities, I was captivated by the wealth and power inequality exacerbated by artificial intelligence (AI). Unequal access to power has existed in our societies for centuries. But today, with an automated tool that predicts people at their most vulnerable moments. If someone is to understand, this is recognized as a problem. And finally, I're a society will have succeeded, according to these god-trick observers from nowhere as Donna Haraway would call them. Data-sense and Non-sense also discusses the nature of the quantified data, describing people who are so connected with sensors and tools to monitor their unconscious signals that they almost exist solely to record and respond to that data. This leads me to the idea of the medical-time visualization or political relationships I've decided that machine learning is not designed`,
  `On the bias), the author talks about how researchers combine intersectional theories and data-driven methods to understand why Black and Brown communities are disproportionately impacted by COVID-19. As the author describes the limitations of certain research, especially one by the Brigham Health team that only looked into the most "limited" definition of intersectionality, I wondered how these researchers applied intersectional theories in their data-driven methodology - one has a more philosophical and human-interest approach while the other is mathematical. The article showed that most of these attempts are flawed because the definitions used to develop the research strategies are "incomplete." Intersectionality also exists alongside power and oppression, and these concepts are difficult, if not hidden, to measure. This makes me wonder if a data-driven approach is the right method to understand a phenomenon that has high human values, but Is that possible? The question of this approach is a big question that I hope to think more about!   I read Kemi Adeyemi's Beyond 90°: the angularities of black/queer/women/lean, and I found it an interesting exploration into how body language and how someone carries themselves is socially and politically significant. The use of non-vertical, non-horizontal physicality`,
];
